{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9ced7e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[init] Using Kubernetes version: v1.23.0\n",
      "[preflight] Running pre-flight checks\n",
      "[preflight] Pulling images required for setting up a Kubernetes cluster\n",
      "[preflight] This might take a minute or two, depending on the speed of your internet connection\n",
      "[preflight] You can also perform this action in beforehand using 'kubeadm config images pull'\n",
      "[certs] Using certificateDir folder \"/etc/kubernetes/pki\"\n",
      "[certs] Generating \"ca\" certificate and key\n",
      "[certs] Generating \"apiserver\" certificate and key\n",
      "[certs] apiserver serving cert is signed for DNS names [k8s-control kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 10.0.10.10]\n",
      "[certs] Generating \"apiserver-kubelet-client\" certificate and key\n",
      "[certs] Generating \"front-proxy-ca\" certificate and key\n",
      "[certs] Generating \"front-proxy-client\" certificate and key\n",
      "[certs] Generating \"etcd/ca\" certificate and key\n",
      "[certs] Generating \"etcd/server\" certificate and key\n",
      "[certs] etcd/server serving cert is signed for DNS names [k8s-control localhost] and IPs [10.0.10.10 127.0.0.1 ::1]\n",
      "[certs] Generating \"etcd/peer\" certificate and key\n",
      "[certs] etcd/peer serving cert is signed for DNS names [k8s-control localhost] and IPs [10.0.10.10 127.0.0.1 ::1]\n",
      "[certs] Generating \"etcd/healthcheck-client\" certificate and key\n",
      "[certs] Generating \"apiserver-etcd-client\" certificate and key\n",
      "[certs] Generating \"sa\" key and public key\n",
      "[kubeconfig] Using kubeconfig folder \"/etc/kubernetes\"\n",
      "[kubeconfig] Writing \"admin.conf\" kubeconfig file\n",
      "[kubeconfig] Writing \"kubelet.conf\" kubeconfig file\n",
      "[kubeconfig] Writing \"controller-manager.conf\" kubeconfig file\n",
      "[kubeconfig] Writing \"scheduler.conf\" kubeconfig file\n",
      "[kubelet-start] Writing kubelet environment file with flags to file \"/var/lib/kubelet/kubeadm-flags.env\"\n",
      "[kubelet-start] Writing kubelet configuration to file \"/var/lib/kubelet/config.yaml\"\n",
      "[kubelet-start] Starting the kubelet\n",
      "[control-plane] Using manifest folder \"/etc/kubernetes/manifests\"\n",
      "[control-plane] Creating static Pod manifest for \"kube-apiserver\"\n",
      "[control-plane] Creating static Pod manifest for \"kube-controller-manager\"\n",
      "[control-plane] Creating static Pod manifest for \"kube-scheduler\"\n",
      "[etcd] Creating static Pod manifest for local etcd in \"/etc/kubernetes/manifests\"\n",
      "[wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory \"/etc/kubernetes/manifests\". This can take up to 4m0s\n",
      "[apiclient] All control plane components are healthy after 24.504156 seconds\n",
      "[upload-config] Storing the configuration used in ConfigMap \"kubeadm-config\" in the \"kube-system\" Namespace\n",
      "[kubelet] Creating a ConfigMap \"kubelet-config-1.23\" in namespace kube-system with the configuration for the kubelets in the cluster\n",
      "NOTE: The \"kubelet-config-1.23\" naming of the kubelet ConfigMap is deprecated. Once the UnversionedKubeletConfigMap feature gate graduates to Beta the default name will become just \"kubelet-config\". Kubeadm upgrade will handle this transition transparently.\n",
      "[upload-certs] Skipping phase. Please see --upload-certs\n",
      "[mark-control-plane] Marking the node k8s-control as control-plane by adding the labels: [node-role.kubernetes.io/master(deprecated) node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers]\n",
      "[mark-control-plane] Marking the node k8s-control as control-plane by adding the taints [node-role.kubernetes.io/master:NoSchedule]\n",
      "[bootstrap-token] Using token: nv1mpp.28jsesmyfcccfjcx\n",
      "[bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles\n",
      "[bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to get nodes\n",
      "[bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials\n",
      "[bootstrap-token] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token\n",
      "[bootstrap-token] configured RBAC rules to allow certificate rotation for all node client certificates in the cluster\n",
      "[bootstrap-token] Creating the \"cluster-info\" ConfigMap in the \"kube-public\" namespace\n",
      "[kubelet-finalize] Updating \"/etc/kubernetes/kubelet.conf\" to point to a rotatable kubelet client certificate and key\n",
      "[addons] Applied essential addon: CoreDNS\n",
      "[addons] Applied essential addon: kube-proxy\n",
      "\n",
      "Your Kubernetes control-plane has initialized successfully!\n",
      "\n",
      "To start using your cluster, you need to run the following as a regular user:\n",
      "\n",
      "  mkdir -p $HOME/.kube\n",
      "  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config\n",
      "  sudo chown $(id -u):$(id -g) $HOME/.kube/config\n",
      "\n",
      "Alternatively, if you are the root user, you can run:\n",
      "\n",
      "  export KUBECONFIG=/etc/kubernetes/admin.conf\n",
      "\n",
      "You should now deploy a pod network to the cluster.\n",
      "Run \"kubectl apply -f [podnetwork].yaml\" with one of the options listed at:\n",
      "  https://kubernetes.io/docs/concepts/cluster-administration/addons/\n",
      "\n",
      "You can now join any number of control-plane nodes by copying certificate authorities\n",
      "and service account keys on each node and then running the following as root:\n",
      "\n",
      "  kubeadm join 10.0.10.10:6443 --token nv1mpp.28jsesmyfcccfjcx \\\n",
      "\t--discovery-token-ca-cert-hash sha256:7bc0e2ccb0719ef8118036dbd44ae8b241c5279a04d87351c84a2332bdaa39f7 \\\n",
      "\t--control-plane \n",
      "\n",
      "Then you can join any number of worker nodes by running the following on each as root:\n",
      "\n",
      "kubeadm join 10.0.10.10:6443 --token nv1mpp.28jsesmyfcccfjcx \\\n",
      "\t--discovery-token-ca-cert-hash sha256:7bc0e2ccb0719ef8118036dbd44ae8b241c5279a04d87351c84a2332bdaa39f7 \n"
     ]
    }
   ],
   "source": [
    "!kubeadm init --pod-network-cidr 192.168.0.0/16 --kubernetes-version 1.23.0 --apiserver-advertise-address 10.0.10.10 --control-plane-endpoint 10.0.10.10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "882b1fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p $HOME/.kube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f663b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp -i /etc/kubernetes/admin.conf $HOME/.kube/config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df603b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "!chown $(id -u):$(id -g) $HOME/.kube/config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4245c6cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME          STATUS     ROLES                  AGE   VERSION\r\n",
      "k8s-control   NotReady   control-plane,master   21s   v1.23.0\r\n"
     ]
    }
   ],
   "source": [
    "!kubectl get nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "efce554e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "configmap/calico-config created\n",
      "customresourcedefinition.apiextensions.k8s.io/bgpconfigurations.crd.projectcalico.org created\n",
      "customresourcedefinition.apiextensions.k8s.io/bgppeers.crd.projectcalico.org created\n",
      "customresourcedefinition.apiextensions.k8s.io/blockaffinities.crd.projectcalico.org created\n",
      "customresourcedefinition.apiextensions.k8s.io/caliconodestatuses.crd.projectcalico.org created\n",
      "customresourcedefinition.apiextensions.k8s.io/clusterinformations.crd.projectcalico.org created\n",
      "customresourcedefinition.apiextensions.k8s.io/felixconfigurations.crd.projectcalico.org created\n",
      "customresourcedefinition.apiextensions.k8s.io/globalnetworkpolicies.crd.projectcalico.org created\n",
      "customresourcedefinition.apiextensions.k8s.io/globalnetworksets.crd.projectcalico.org created\n",
      "customresourcedefinition.apiextensions.k8s.io/hostendpoints.crd.projectcalico.org created\n",
      "customresourcedefinition.apiextensions.k8s.io/ipamblocks.crd.projectcalico.org created\n",
      "customresourcedefinition.apiextensions.k8s.io/ipamconfigs.crd.projectcalico.org created\n",
      "customresourcedefinition.apiextensions.k8s.io/ipamhandles.crd.projectcalico.org created\n",
      "customresourcedefinition.apiextensions.k8s.io/ippools.crd.projectcalico.org created\n",
      "customresourcedefinition.apiextensions.k8s.io/ipreservations.crd.projectcalico.org created\n",
      "customresourcedefinition.apiextensions.k8s.io/kubecontrollersconfigurations.crd.projectcalico.org created\n",
      "customresourcedefinition.apiextensions.k8s.io/networkpolicies.crd.projectcalico.org created\n",
      "customresourcedefinition.apiextensions.k8s.io/networksets.crd.projectcalico.org created\n",
      "clusterrole.rbac.authorization.k8s.io/calico-kube-controllers created\n",
      "clusterrolebinding.rbac.authorization.k8s.io/calico-kube-controllers created\n",
      "clusterrole.rbac.authorization.k8s.io/calico-node created\n",
      "clusterrolebinding.rbac.authorization.k8s.io/calico-node created\n",
      "daemonset.apps/calico-node created\n",
      "serviceaccount/calico-node created\n",
      "deployment.apps/calico-kube-controllers created\n",
      "serviceaccount/calico-kube-controllers created\n",
      "poddisruptionbudget.policy/calico-kube-controllers created\n"
     ]
    }
   ],
   "source": [
    "!kubectl apply -f https://docs.projectcalico.org/manifests/calico.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1db35e52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME          STATUS   ROLES                  AGE   VERSION\r\n",
      "k8s-control   Ready    control-plane,master   71s   v1.23.0\r\n"
     ]
    }
   ],
   "source": [
    "!kubectl get nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0986f709",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                                       READY   STATUS              RESTARTS   AGE\r\n",
      "calico-kube-controllers-7bc6547ffb-r5flk   0/1     ContainerCreating   0          68s\r\n",
      "calico-node-gt52j                          0/1     PodInitializing     0          68s\r\n",
      "coredns-64897985d-lxsgt                    0/1     ContainerCreating   0          75s\r\n",
      "coredns-64897985d-sdrkn                    0/1     ContainerCreating   0          75s\r\n",
      "etcd-k8s-control                           1/1     Running             0          85s\r\n",
      "kube-apiserver-k8s-control                 1/1     Running             0          84s\r\n",
      "kube-controller-manager-k8s-control        1/1     Running             0          85s\r\n",
      "kube-proxy-hn75t                           1/1     Running             0          75s\r\n",
      "kube-scheduler-k8s-control                 1/1     Running             0          85s\r\n"
     ]
    }
   ],
   "source": [
    "!kubectl get pods -n kube-system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "455117bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name:                 calico-node-gt52j\r\n",
      "Namespace:            kube-system\r\n",
      "Priority:             2000001000\r\n",
      "Priority Class Name:  system-node-critical\r\n",
      "Node:                 k8s-control/10.0.10.10\r\n",
      "Start Time:           Mon, 18 Jul 2022 07:51:59 +0000\r\n",
      "Labels:               controller-revision-hash=7645d9b67c\r\n",
      "                      k8s-app=calico-node\r\n",
      "                      pod-template-generation=1\r\n",
      "Annotations:          <none>\r\n",
      "Status:               Running\r\n",
      "IP:                   10.0.10.10\r\n",
      "IPs:\r\n",
      "  IP:           10.0.10.10\r\n",
      "Controlled By:  DaemonSet/calico-node\r\n",
      "Init Containers:\r\n",
      "  upgrade-ipam:\r\n",
      "    Container ID:  containerd://37463bf0a1d20c2206a6c22d8b69791193cac668067dbd1bdc5541606094e9e1\r\n",
      "    Image:         docker.io/calico/cni:v3.23.2\r\n",
      "    Image ID:      docker.io/calico/cni@sha256:a5c1e5eba59b0b59b6767fbcfb6d4b2cef355a83cfc0627b31e7f4dddfec09d0\r\n",
      "    Port:          <none>\r\n",
      "    Host Port:     <none>\r\n",
      "    Command:\r\n",
      "      /opt/cni/bin/calico-ipam\r\n",
      "      -upgrade\r\n",
      "    State:          Terminated\r\n",
      "      Reason:       Completed\r\n",
      "      Exit Code:    0\r\n",
      "      Started:      Mon, 18 Jul 2022 07:52:30 +0000\r\n",
      "      Finished:     Mon, 18 Jul 2022 07:52:30 +0000\r\n",
      "    Ready:          True\r\n",
      "    Restart Count:  0\r\n",
      "    Environment Variables from:\r\n",
      "      kubernetes-services-endpoint  ConfigMap  Optional: true\r\n",
      "    Environment:\r\n",
      "      KUBERNETES_NODE_NAME:        (v1:spec.nodeName)\r\n",
      "      CALICO_NETWORKING_BACKEND:  <set to the key 'calico_backend' of config map 'calico-config'>  Optional: false\r\n",
      "    Mounts:\r\n",
      "      /host/opt/cni/bin from cni-bin-dir (rw)\r\n",
      "      /var/lib/cni/networks from host-local-net-dir (rw)\r\n",
      "      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-2j4bf (ro)\r\n",
      "  install-cni:\r\n",
      "    Container ID:  containerd://8ddd22834b94138ac95c38d778f6704db1d24410f1ca0dffdfa3decdef891f59\r\n",
      "    Image:         docker.io/calico/cni:v3.23.2\r\n",
      "    Image ID:      docker.io/calico/cni@sha256:a5c1e5eba59b0b59b6767fbcfb6d4b2cef355a83cfc0627b31e7f4dddfec09d0\r\n",
      "    Port:          <none>\r\n",
      "    Host Port:     <none>\r\n",
      "    Command:\r\n",
      "      /opt/cni/bin/install\r\n",
      "    State:          Terminated\r\n",
      "      Reason:       Completed\r\n",
      "      Exit Code:    0\r\n",
      "      Started:      Mon, 18 Jul 2022 07:52:31 +0000\r\n",
      "      Finished:     Mon, 18 Jul 2022 07:52:40 +0000\r\n",
      "    Ready:          True\r\n",
      "    Restart Count:  0\r\n",
      "    Environment Variables from:\r\n",
      "      kubernetes-services-endpoint  ConfigMap  Optional: true\r\n",
      "    Environment:\r\n",
      "      CNI_CONF_NAME:         10-calico.conflist\r\n",
      "      CNI_NETWORK_CONFIG:    <set to the key 'cni_network_config' of config map 'calico-config'>  Optional: false\r\n",
      "      KUBERNETES_NODE_NAME:   (v1:spec.nodeName)\r\n",
      "      CNI_MTU:               <set to the key 'veth_mtu' of config map 'calico-config'>  Optional: false\r\n",
      "      SLEEP:                 false\r\n",
      "    Mounts:\r\n",
      "      /host/etc/cni/net.d from cni-net-dir (rw)\r\n",
      "      /host/opt/cni/bin from cni-bin-dir (rw)\r\n",
      "      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-2j4bf (ro)\r\n",
      "Containers:\r\n",
      "  calico-node:\r\n",
      "    Container ID:   containerd://57d745b1289224a60bc65249fdd62fbe33ee536acdc81c38f269361ecb52d9f5\r\n",
      "    Image:          docker.io/calico/node:v3.23.2\r\n",
      "    Image ID:       docker.io/calico/node@sha256:b4ac0660c297b3a582ef2f4a0d7ef86f954ad5497b704b41d82fa99418e7a51e\r\n",
      "    Port:           <none>\r\n",
      "    Host Port:      <none>\r\n",
      "    State:          Running\r\n",
      "      Started:      Mon, 18 Jul 2022 07:53:07 +0000\r\n",
      "    Ready:          True\r\n",
      "    Restart Count:  0\r\n",
      "    Requests:\r\n",
      "      cpu:      250m\r\n",
      "    Liveness:   exec [/bin/calico-node -felix-live -bird-live] delay=10s timeout=10s period=10s #success=1 #failure=6\r\n",
      "    Readiness:  exec [/bin/calico-node -felix-ready -bird-ready] delay=0s timeout=10s period=10s #success=1 #failure=3\r\n",
      "    Environment Variables from:\r\n",
      "      kubernetes-services-endpoint  ConfigMap  Optional: true\r\n",
      "    Environment:\r\n",
      "      DATASTORE_TYPE:                     kubernetes\r\n",
      "      WAIT_FOR_DATASTORE:                 true\r\n",
      "      NODENAME:                            (v1:spec.nodeName)\r\n",
      "      CALICO_NETWORKING_BACKEND:          <set to the key 'calico_backend' of config map 'calico-config'>  Optional: false\r\n",
      "      CLUSTER_TYPE:                       k8s,bgp\r\n",
      "      IP:                                 autodetect\r\n",
      "      CALICO_IPV4POOL_IPIP:               Always\r\n",
      "      CALICO_IPV4POOL_VXLAN:              Never\r\n",
      "      CALICO_IPV6POOL_VXLAN:              Never\r\n",
      "      FELIX_IPINIPMTU:                    <set to the key 'veth_mtu' of config map 'calico-config'>  Optional: false\r\n",
      "      FELIX_VXLANMTU:                     <set to the key 'veth_mtu' of config map 'calico-config'>  Optional: false\r\n",
      "      FELIX_WIREGUARDMTU:                 <set to the key 'veth_mtu' of config map 'calico-config'>  Optional: false\r\n",
      "      CALICO_DISABLE_FILE_LOGGING:        true\r\n",
      "      FELIX_DEFAULTENDPOINTTOHOSTACTION:  ACCEPT\r\n",
      "      FELIX_IPV6SUPPORT:                  false\r\n",
      "      FELIX_HEALTHENABLED:                true\r\n",
      "    Mounts:\r\n",
      "      /host/etc/cni/net.d from cni-net-dir (rw)\r\n",
      "      /lib/modules from lib-modules (ro)\r\n",
      "      /run/xtables.lock from xtables-lock (rw)\r\n",
      "      /sys/fs/ from sysfs (rw)\r\n",
      "      /var/lib/calico from var-lib-calico (rw)\r\n",
      "      /var/log/calico/cni from cni-log-dir (ro)\r\n",
      "      /var/run/calico from var-run-calico (rw)\r\n",
      "      /var/run/nodeagent from policysync (rw)\r\n",
      "      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-2j4bf (ro)\r\n",
      "Conditions:\r\n",
      "  Type              Status\r\n",
      "  Initialized       True \r\n",
      "  Ready             True \r\n",
      "  ContainersReady   True \r\n",
      "  PodScheduled      True \r\n",
      "Volumes:\r\n",
      "  lib-modules:\r\n",
      "    Type:          HostPath (bare host directory volume)\r\n",
      "    Path:          /lib/modules\r\n",
      "    HostPathType:  \r\n",
      "  var-run-calico:\r\n",
      "    Type:          HostPath (bare host directory volume)\r\n",
      "    Path:          /var/run/calico\r\n",
      "    HostPathType:  \r\n",
      "  var-lib-calico:\r\n",
      "    Type:          HostPath (bare host directory volume)\r\n",
      "    Path:          /var/lib/calico\r\n",
      "    HostPathType:  \r\n",
      "  xtables-lock:\r\n",
      "    Type:          HostPath (bare host directory volume)\r\n",
      "    Path:          /run/xtables.lock\r\n",
      "    HostPathType:  FileOrCreate\r\n",
      "  sysfs:\r\n",
      "    Type:          HostPath (bare host directory volume)\r\n",
      "    Path:          /sys/fs/\r\n",
      "    HostPathType:  DirectoryOrCreate\r\n",
      "  cni-bin-dir:\r\n",
      "    Type:          HostPath (bare host directory volume)\r\n",
      "    Path:          /opt/cni/bin\r\n",
      "    HostPathType:  \r\n",
      "  cni-net-dir:\r\n",
      "    Type:          HostPath (bare host directory volume)\r\n",
      "    Path:          /etc/cni/net.d\r\n",
      "    HostPathType:  \r\n",
      "  cni-log-dir:\r\n",
      "    Type:          HostPath (bare host directory volume)\r\n",
      "    Path:          /var/log/calico/cni\r\n",
      "    HostPathType:  \r\n",
      "  host-local-net-dir:\r\n",
      "    Type:          HostPath (bare host directory volume)\r\n",
      "    Path:          /var/lib/cni/networks\r\n",
      "    HostPathType:  \r\n",
      "  policysync:\r\n",
      "    Type:          HostPath (bare host directory volume)\r\n",
      "    Path:          /var/run/nodeagent\r\n",
      "    HostPathType:  DirectoryOrCreate\r\n",
      "  kube-api-access-2j4bf:\r\n",
      "    Type:                    Projected (a volume that contains injected data from multiple sources)\r\n",
      "    TokenExpirationSeconds:  3607\r\n",
      "    ConfigMapName:           kube-root-ca.crt\r\n",
      "    ConfigMapOptional:       <nil>\r\n",
      "    DownwardAPI:             true\r\n",
      "QoS Class:                   Burstable\r\n",
      "Node-Selectors:              kubernetes.io/os=linux\r\n",
      "Tolerations:                 :NoSchedule op=Exists\r\n",
      "                             :NoExecute op=Exists\r\n",
      "                             CriticalAddonsOnly op=Exists\r\n",
      "                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists\r\n",
      "                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists\r\n",
      "                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists\r\n",
      "                             node.kubernetes.io/not-ready:NoExecute op=Exists\r\n",
      "                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists\r\n",
      "                             node.kubernetes.io/unreachable:NoExecute op=Exists\r\n",
      "                             node.kubernetes.io/unschedulable:NoSchedule op=Exists\r\n",
      "Events:\r\n",
      "  Type     Reason     Age                From               Message\r\n",
      "  ----     ------     ----               ----               -------\r\n",
      "  Normal   Scheduled  21m                default-scheduler  Successfully assigned kube-system/calico-node-gt52j to k8s-control\r\n",
      "  Normal   Pulling    21m                kubelet            Pulling image \"docker.io/calico/cni:v3.23.2\"\r\n",
      "  Normal   Pulled     21m                kubelet            Successfully pulled image \"docker.io/calico/cni:v3.23.2\" in 25.149414156s\r\n",
      "  Normal   Created    21m                kubelet            Created container upgrade-ipam\r\n",
      "  Normal   Started    21m                kubelet            Started container upgrade-ipam\r\n",
      "  Normal   Pulled     21m                kubelet            Container image \"docker.io/calico/cni:v3.23.2\" already present on machine\r\n",
      "  Normal   Created    21m                kubelet            Created container install-cni\r\n",
      "  Normal   Started    21m                kubelet            Started container install-cni\r\n",
      "  Normal   Pulling    20m                kubelet            Pulling image \"docker.io/calico/node:v3.23.2\"\r\n",
      "  Normal   Pulled     20m                kubelet            Successfully pulled image \"docker.io/calico/node:v3.23.2\" in 21.251511998s\r\n",
      "  Normal   Created    20m                kubelet            Created container calico-node\r\n",
      "  Normal   Started    20m                kubelet            Started container calico-node\r\n",
      "  Warning  Unhealthy  20m                kubelet            Readiness probe failed: calico/node is not ready: BIRD is not ready: Error querying BIRD: unable to connect to BIRDv4 socket: dial unix /var/run/bird/bird.ctl: connect: no such file or directory\r\n",
      "  Warning  Unhealthy  20m (x2 over 20m)  kubelet            Readiness probe failed: calico/node is not ready: BIRD is not ready: Error querying BIRD: unable to connect to BIRDv4 socket: dial unix /var/run/calico/bird.ctl: connect: connection refused\r\n",
      "  Warning  Unhealthy  16m                kubelet            Readiness probe failed: 2022-07-18 07:57:09.592 [INFO][991] confd/health.go 180: Number of node(s) with BGP peering established = 0\r\n",
      "calico/node is not ready: BIRD is not ready: BGP not established with 10.0.10.11\r\n"
     ]
    }
   ],
   "source": [
    "!kubectl describe pods calico-node-gt52j -n kube-system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c52b5f9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME          STATUS   ROLES                  AGE   VERSION\r\n",
      "k8s-control   Ready    control-plane,master   31m   v1.23.0\r\n",
      "k8s-worker1   Ready    <none>                 27m   v1.23.0\r\n"
     ]
    }
   ],
   "source": [
    "!kubectl get nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb129de4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
